<!DOCTYPE html>
<html>
<head>
  <title>Real-time Video Analysis</title>
  <style>
    #video {
      width: 100%;
      aspect-ratio: 16 / 9;
    }
    #description {
      border: 1px solid gray;
      padding: 10px;
      margin-top: 10px;
      font-family: Arial, sans-serif;
      font-size: 14px;
      line-height: 1.5;
    }
    #canvas {
      display: none; /* For debugging, set to block if needed */
    }
  </style>
</head>
<body>
  <button id="startButton" style="width: 100%;">Start Camera</button>
  <button id="switchCamera" style="width: 48%; margin-right: 4%;">Switch Camera</button>
  <button id="monitorButton" style="width: 48%;">Monitor</button>
  <video id="video" autoplay muted></video>
  <div id="description">Detected Objects:</div>
  <canvas id="canvas"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://docs.opencv.org/4.x/opencv.js"></script>
  <script>
    const startButton = document.getElementById('startButton');
    const switchCameraButton = document.getElementById('switchCamera');
    const monitorButton = document.getElementById('monitorButton');
    const video = document.getElementById('video');
    const description = document.getElementById('description');
    const canvas = document.getElementById('canvas');

    let facingMode = 'user'; // Default to front camera
    let stream = null;
    let model = null;

    console.log("Initializing script...");

    // Load TensorFlow model
    cocoSsd.load()
      .then(loadedModel => {
        model = loadedModel;
        console.log("COCO-SSD model loaded successfully.");
      })
      .catch(err => console.error("Error loading COCO-SSD model:", err));

    startButton.addEventListener('click', () => {
      console.log('Starting camera...');
      navigator.mediaDevices.getUserMedia({ video: { facingMode: facingMode } })
        .then(newStream => {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
          }
          stream = newStream;
          video.srcObject = stream;
          video.play();

          video.addEventListener('loadeddata', () => {
            console.log("Video stream loaded. Video dimensions:", video.videoWidth, "x", video.videoHeight);
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            cv.onRuntimeInitialized = () => {
              console.log("OpenCV.js initialized successfully.");
            };
          });
        })
        .catch(error => console.error("Error accessing camera:", error));
    });

    switchCameraButton.addEventListener('click', () => {
      console.log("Switching camera...");
      facingMode = facingMode === "user" ? "environment" : "user";
      startButton.click();
    });

    monitorButton.addEventListener('click', () => {
      console.log("Monitoring started...");
      if (!model) {
        console.error("COCO-SSD model not loaded yet.");
        return;
      }
      if (!video.srcObject) {
        console.error("Video stream not started yet.");
        return;
      }
      monitorVideo();
    });

    function monitorVideo() {
      console.log("MonitorVideo function started...");

      const cap = new cv.VideoCapture(video);

      function analyzeFrame() {
        try {
          // Dynamically create the matrix to ensure matching dimensions
          const videoWidth = video.videoWidth;
          const videoHeight = video.videoHeight;
          if (!videoWidth || !videoHeight) {
            console.error("Video dimensions unavailable.");
            requestAnimationFrame(analyzeFrame);
            return;
          }

          // Recreate frame matrix with correct dimensions
          const frame = new cv.Mat(videoHeight, videoWidth, cv.CV_8UC4);

          console.log(`Analyzing frame (${videoWidth}x${videoHeight})...`);
          cap.read(frame); // Read current video frame
          console.log("Frame captured.");

          // Convert frame to TensorFlow tensor
          const tensor = tf.browser.fromPixels(frame);
          console.log("Frame converted to TensorFlow tensor.");

          // Perform object detection
          model.detect(tensor).then(predictions => {
            console.log("Object detection complete:", predictions);

            // Update description
            description.textContent = predictions.map(p => `${p.class} (${p.score.toFixed(2)})`).join(", ") || "No objects detected.";

            // Draw bounding boxes on the frame
            predictions.forEach(prediction => {
              const [x, y, width, height] = prediction.bbox;
              cv.rectangle(frame, new cv.Point(x, y), new cv.Point(x + width, y + height), [0, 255, 0, 255], 2);
              cv.putText(frame, prediction.class, new cv.Point(x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, [255, 0, 0, 255], 2);
            });

            cv.imshow(canvas, frame);
            console.log("Bounding boxes drawn on frame.");
          }).catch(err => {
            console.error("Error during object detection:", err);
          });

          tensor.dispose(); // Release memory
          frame.delete(); // Clean up OpenCV matrix
        } catch (err) {
          console.error("Error during frame analysis:", err);
        }

        // Process next frame
        requestAnimationFrame(analyzeFrame);
      }

      // Start processing frames
      requestAnimationFrame(analyzeFrame);
    }
  </script>
</body>
</html>
