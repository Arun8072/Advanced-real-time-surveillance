<!DOCTYPE html>
<html>
<head>
  <title>Real-time Video Analysis with NLP</title>
  <style>
    #video {
      width: 100%;
      aspect-ratio: 16 / 9;
    }
    #description {
      border: 1px solid gray;
      padding: 10px;
      margin-top: 10px;
      font-family: Arial, sans-serif;
      font-size: 14px;
      line-height: 1.5;
    }
    #canvas {
      display: none;
    }
  </style>
</head>
<body>
  <button id="startButton" style="width: 100%;">Start Camera</button>
  <button id="switchCamera" style="width: 48%; margin-right: 4%;">Switch Camera</button>
  <button id="monitorButton" style="width: 48%;">Monitor</button>
  <video id="video" autoplay muted></video>
  <div id="description">Detected Objects and Description:</div>
  <canvas id="canvas"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://docs.opencv.org/4.x/opencv.js"></script>
  <script src="https://unpkg.com/compromise@latest/builds/compromise.min.js"></script>
  <script>
    const startButton = document.getElementById('startButton');
    const switchCameraButton = document.getElementById('switchCamera');
    const monitorButton = document.getElementById('monitorButton');
    const video = document.getElementById('video');
    const description = document.getElementById('description');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    let facingMode = 'user'; // Default to front camera
    let stream = null;
    let model = null;

    // Load the COCO-SSD model
    cocoSsd.load().then(loadedModel => {
      model = loadedModel;
      console.log("COCO-SSD model loaded.");
    }).catch(err => console.error("Error loading COCO-SSD model:", err));

    startButton.addEventListener('click', () => {
      console.log('Starting camera...');
      navigator.mediaDevices.getUserMedia({ video: { facingMode: facingMode } })
        .then(newStream => {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
          }
          stream = newStream;
          video.srcObject = stream;
          video.play();

          video.addEventListener('loadeddata', () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            cv.onRuntimeInitialized = () => {
              console.log("OpenCV.js initialized.");
            };
          });
        })
        .catch(error => console.error("Error accessing camera:", error));
    });

    switchCameraButton.addEventListener('click', () => {
      console.log("Switching camera...");
      facingMode = facingMode === "user" ? "environment" : "user";
      startButton.click();
    });

    monitorButton.addEventListener('click', () => {
      console.log("Monitoring started...");
      if (!model) {
        console.error("COCO-SSD model not loaded yet.");
        return;
      }
      monitorVideo();
    });

    function monitorVideo() {
      const cap = new cv.VideoCapture(video);
      const frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);

      function analyzeFrame() {
        cap.read(frame);
        cv.imshow(canvas, frame);

        // Convert frame to TensorFlow tensor
        const tensor = tf.browser.fromPixels(canvas);
        model.detect(tensor).then(predictions => {
          tensor.dispose(); // Dispose tensor to save memory

          // Generate NLP description
          const descriptionText = generateNLPDescription(predictions);
          description.textContent = descriptionText;

          // Draw bounding boxes and labels
          predictions.forEach(prediction => {
            const [x, y, width, height] = prediction.bbox;
            cv.rectangle(frame, new cv.Point(x, y), new cv.Point(x + width, y + height), [0, 255, 0, 255], 2);
            cv.putText(frame, prediction.class, new cv.Point(x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, [255, 0, 0, 255], 2);
          });

          cv.imshow(canvas, frame);
        });

        requestAnimationFrame(analyzeFrame);
      }

      requestAnimationFrame(analyzeFrame);
    }

    function generateNLPDescription(predictions) {
      if (predictions.length === 0) {
        return "No objects detected in the current frame.";
      }

      // Extract object classes and build sentences
      const classes = predictions.map(prediction => prediction.class);
      let descriptionText = `The camera sees ${classes.length} objects: ${classes.join(", ")}.`;

      // Process text with Compromise.js for variations
      let nlp = nlp(descriptionText);
      const enhancedDescription = nlp.sentences().toNegative().out();
      
      return enhancedDescription || descriptionText;
    }
  </script>
</body>
</html>
