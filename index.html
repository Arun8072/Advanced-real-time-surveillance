<!DOCTYPE html>
<html>
<head>
  <title>Real-time Video Analysis with NLP</title>
  <style>
    #video {
      width: 100%;
      aspect-ratio: 16 / 9;
    }
    #description {
      border: 1px solid gray;
      padding: 10px;
      margin-top: 10px;
      font-family: Arial, sans-serif;
      font-size: 14px;
      line-height: 1.5;
    }
    #canvas {
      display: none;
    }
  </style>
</head>
<body>
  <button id="startButton" style="width: 100%;">Start Camera</button>
  <button id="switchCamera" style="width: 48%; margin-right: 4%;">Switch Camera</button>
  <button id="monitorButton" style="width: 48%;">Monitor</button>
  <video id="video" autoplay muted></video>
  <div id="description">Detected Objects and Description:</div>
  <canvas id="canvas"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://docs.opencv.org/4.x/opencv.js"></script>
  <script src="https://unpkg.com/compromise@latest/builds/compromise.min.js"></script>
  <script>
    const startButton = document.getElementById('startButton');
    const switchCameraButton = document.getElementById('switchCamera');
    const monitorButton = document.getElementById('monitorButton');
    const video = document.getElementById('video');
    const description = document.getElementById('description');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    let facingMode = 'user'; // Default to front camera
    let stream = null;
    let model = null;

    console.log("Script loaded. Waiting for user actions...");

    // Load the COCO-SSD model
    cocoSsd.load()
      .then(loadedModel => {
        model = loadedModel;
        console.log("COCO-SSD model loaded successfully.");
      })
      .catch(err => {
        console.error("Error loading COCO-SSD model:", err);
      });

    startButton.addEventListener('click', () => {
      console.log('Start button clicked. Attempting to access camera...');
      navigator.mediaDevices.getUserMedia({ video: { facingMode: facingMode } })
        .then(newStream => {
          console.log("Camera access granted. Setting up video stream.");
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
            console.log("Stopped previous video stream.");
          }
          stream = newStream;
          video.srcObject = stream;
          video.play();

          video.addEventListener('loadeddata', () => {
            console.log("Video stream loaded successfully.");
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            cv.onRuntimeInitialized = () => {
              console.log("OpenCV.js initialized successfully.");
            };
          });
        })
        .catch(error => console.error("Error accessing camera:", error));
    });

    switchCameraButton.addEventListener('click', () => {
      console.log("Switching camera...");
      facingMode = facingMode === "user" ? "environment" : "user";
      startButton.click();
    });

    monitorButton.addEventListener('click', () => {
      console.log("Monitor button clicked. Checking readiness...");
      if (!model) {
        console.error("COCO-SSD model not loaded yet.");
        return;
      }
      if (!video.srcObject) {
        console.error("Video stream not started yet.");
        return;
      }
      console.log("Starting monitoring...");
      monitorVideo();
    });

    function monitorVideo() {
  console.log("MonitorVideo function started. Preparing OpenCV capture...");

  // Ensure the video stream dimensions are set
  const videoWidth = video.videoWidth;
  const videoHeight = video.videoHeight;

  if (!videoWidth || !videoHeight) {
    console.error("Video dimensions are not available.");
    return;
  }

  // Initialize the OpenCV VideoCapture and matrix with correct dimensions
  const cap = new cv.VideoCapture(video);
  const frame = new cv.Mat(videoHeight, videoWidth, cv.CV_8UC4); // Ensure matching dimensions

  console.log(`Initialized OpenCV VideoCapture with video dimensions: ${videoWidth}x${videoHeight}`);

  function analyzeFrame() {
    console.log("Analyzing frame...");

    try {
      cap.read(frame); // Read the video frame into the matrix
      console.log("Frame captured successfully.");
      
      // Display the frame on the canvas for debugging
      cv.imshow(canvas, frame);
      console.log("Frame displayed on canvas.");

      // Convert the frame to TensorFlow tensor
      const tensor = tf.browser.fromPixels(canvas);
      console.log("Frame converted to tensor.");

      // Run object detection
      model.detect(tensor).then(predictions => {
        console.log("Object detection completed. Predictions received:", predictions);

        // Generate NLP description
        const descriptionText = generateNLPDescription(predictions);
        description.textContent = descriptionText;
        console.log("Description updated:", descriptionText);

        // Draw bounding boxes on the frame
        predictions.forEach(prediction => {
          const [x, y, width, height] = prediction.bbox;
          cv.rectangle(frame, new cv.Point(x, y), new cv.Point(x + width, y + height), [0, 255, 0, 255], 2);
          cv.putText(frame, prediction.class, new cv.Point(x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, [255, 0, 0, 255], 2);
        });
        cv.imshow(canvas, frame);
        console.log("Bounding boxes drawn.");
      }).catch(err => {
        console.error("Error during object detection:", err);
      });

      tensor.dispose(); // Dispose tensor to save memory
    } catch (err) {
      console.error("Error during frame analysis:", err);
    }

    // Process the next frame
    requestAnimationFrame(analyzeFrame);
  }

  // Start analyzing frames
  requestAnimationFrame(analyzeFrame);
}


    function generateNLPDescription(predictions) {
      console.log("Generating NLP description...");
      if (predictions.length === 0) {
        return "No objects detected in the current frame.";
      }

      // Extract object classes and build sentences
      const classes = predictions.map(prediction => prediction.class);
      let descriptionText = `The camera sees ${classes.length} objects: ${classes.join(", ")}.`;

      // Process text with Compromise.js for variations
      let nlp = nlp(descriptionText);
      const enhancedDescription = nlp.sentences().toNegative().out();

      console.log("NLP description generated.");
      return enhancedDescription || descriptionText;
    }

    
});

  </script>
</body>
</html>
