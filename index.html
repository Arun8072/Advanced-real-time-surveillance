<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Real-time Object, Pose, Face, Behavior, and Statistical Anomaly Detection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f0f0f0;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        h1 {
            text-align: center;
            color: #333;
        }

        .video-container {
            position: relative;
            width: 100%;
            max-width: 640px;
            margin: 0 auto;
            aspect-ratio: 4 / 3;
            background-color: #000;
        }

        #video, #canvas, #placeholder-canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: contain;
        }

        .controls {
            display: flex;
            justify-content: center;
            margin-top: 20px;
            gap: 10px;
        }

        button {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 5px;
        }

        button:hover {
            background-color: #45a049;
        }

        #total_obj, #detectionResults, #log, #nlpDescription {
            margin-top: 20px;
            padding: 10px;
            background-color: white;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }

        #total_obj {
            font-size: 18px;
            font-weight: bold;
            text-align: center;
        }

        #log, #nlpDescription {
            position: relative;
            white-space: pre-wrap;
            font-family: monospace;
            font-size: 14px;
            overflow-y: auto;
            padding-bottom: 30px; /* Space for the expand button */
            max-height: 200px; /* Set a fixed height */
        }

        .expand-btn {
            position: absolute;
            bottom: 5px;
            right: 5px;
            background-color: rgba(0, 0, 0, 0.5);
            color: white;
            border: none;
            padding: 5px 10px;
            cursor: pointer;
            font-size: 12px;
            border-radius: 3px;
            z-index: 10; /* Ensure the button is always on top */
        }

        .expanded {
            max-height: none !important;
        }
        .log-container {
            position: relative;
        }
        .log-content {
            overflow-y: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Advanced Real-time Object, Pose, Face, Behavior, and Statistical Anomaly Detection</h1>
        <div class="video-container">
            <video id="video" playsinline></video>
            <canvas id="canvas"></canvas>
            <canvas id="placeholder-canvas"></canvas>
        </div>
        <div class="controls">
            <button id="toggleCamera">Start Camera</button>
            <button id="switchCamera">Switch Camera</button>
            <button id="startDetection">Start Detection</button>
        </div>
        <div id="total_obj">Total Detected <span id="obj_count">0</span> objects</div>
        <div id="detectionResults"></div>
        <div id="nlpDescription" class="log-container">
            <div class="log-content"></div>
            <button class="expand-btn">Expand</button>
        </div>
        <div id="log" class="log-container">
            <div class="log-content"></div>
            <button class="expand-btn">Expand</button>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
    <script src="https://cdn.jsdelivr.net/npm/simple-statistics@7.8.0/dist/simple-statistics.min.js"></script>
    <script src="https://docs.opencv.org/4.x/opencv.js" onload="onOpenCvReady();" async></script>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/ml5@latest/dist/ml5.min.js"></script>
    <script>
        let video, canvas, ctx, placeholderCanvas, placeholderCtx, objectDetectionModel, poseNetModel, stream;
        let isDetecting = false;
        let currentCameraFacingMode = 'environment';
        const objCountSpan = document.getElementById('obj_count');
        const nlpDescriptionDiv = document.getElementById('nlpDescription');
        let uniqueObjects = new Set();
        let prevObjects = [];
        let objectTracker = {};
        let gestureHistory = [];
        let motionHistory = [];
        let speedHistory = {};
        let behaviorHistory = []; // Updated initialization
        let positionHistory = []; // Added positionHistory
        let restrictedAreas = [
            { x: 100, y: 100, width: 200, height: 150 },
            { x: 400, y: 300, width: 150, height: 100 }
        ];

        const toggleCameraButton = document.getElementById('toggleCamera');
        const switchCameraButton = document.getElementById('switchCamera');
        const startDetectionButton = document.getElementById('startDetection');
        const detectionResultsDiv = document.getElementById('detectionResults');
        const logDiv = document.getElementById('log');

        let ocrModel;

        function log(message) {
            console.log(message);
            const logEntry = document.createElement('div');
            logEntry.textContent = `${new Date().toLocaleTimeString()}: ${message}`;
            document.querySelector('#log .log-content').insertBefore(logEntry, document.querySelector('#log .log-content').firstChild);
            if (document.querySelector('#log .log-content').childElementCount > 50) {
                document.querySelector('#log .log-content').removeChild(document.querySelector('#log .log-content').lastChild);
            }
        }

        function updateObjectCount() {
            objCountSpan.textContent = uniqueObjects.size;
        }

        async function initOCRModel() {
            try {
                ocrModel = await ml5.ocr('eng');
                log('ml5.js OCR model initialized successfully');
            } catch (error) {
                log('Error initializing ml5.js OCR model: ' + error.message);
                console.error('Error details:', error);
            }
        }

        async function recognizeLicensePlate(imageData) {
            try {
                const results = await ocrModel.recognize(imageData);
                const text = results.map(result => result.text).join(' ');
                log(`License plate recognized: ${text}`);
                updateNLPDescription(`License plate recognized: ${text}`);
            } catch (error) {
                log('Error in license plate recognition: ' + error.message);
                console.error('Error details:', error);
            }
        }

        async function classifyVehicle(prediction) {
            try {
                // Placeholder for actual vehicle classification logic
                const vehicleTypes = ['car', 'truck', 'motorcycle', 'bus', 'bicycle'];
                const randomIndex = Math.floor(Math.random() * vehicleTypes.length);
                return vehicleTypes[randomIndex];
            } catch (error) {
                log('Error in vehicle classification: ' + error.message);
                return 'unknown';
            }
        }

        function updateNLPDescription(newInfo) {
            const descriptionElement = document.createElement('p');
            descriptionElement.textContent = `${new Date().toLocaleTimeString()}: ${newInfo}`;
            document.querySelector('#nlpDescription .log-content').insertBefore(descriptionElement, document.querySelector('#nlpDescription .log-content').firstChild);
            if (document.querySelector('#nlpDescription .log-content').childElementCount > 50) {
                document.querySelector('#nlpDescription .log-content').removeChild(document.querySelector('#nlpDescription .log-content').lastChild);
            }
        }

        function generateNLPDescription(objects, poses, anomalies, motions, faceResults, behaviors) {
            let description = '';
            const newObjects = objects.filter(obj => !prevObjects.some(prevObj => prevObj.class === obj.class));
            const disappearedObjects = prevObjects.filter(prevObj => !objects.some(obj => obj.class === prevObj.class));

            if (newObjects.length > 0) {
                description += `New objects detected: ${newObjects.map(obj => obj.class).join(', ')}. `;
            }

            if (disappearedObjects.length > 0) {
                description += `Objects no longer visible: ${disappearedObjects.map(obj => obj.class).join(', ')}. `;
            }

            const movingObjects = [];
            objects.forEach(obj => {
                const prevObj = prevObjects.find(pObj => pObj.class === obj.class);
                if (prevObj) {
                    const xDiff = obj.bbox[0] - prevObj.bbox[0];
                    const yDiff = obj.bbox[1] - prevObj.bbox[1];
                    if (Math.abs(xDiff) > 10 || Math.abs(yDiff) > 10) {
                        const xDirection = xDiff > 0 ? 'right' : 'left';
                        const yDirection = yDiff > 0 ? 'down' : 'up';
                        movingObjects.push(`${obj.class} moving ${xDirection} and ${yDirection}`);
                    }
                }

                // Track object movement
                if (!objectTracker[obj.class]) {
                    objectTracker[obj.class] = [];
                }
                objectTracker[obj.class].push({x: obj.bbox[0], y: obj.bbox[1], time: Date.now()});
                if (objectTracker[obj.class].length > 10) {
                    objectTracker[obj.class].shift();
                }
            });

            if (movingObjects.length > 0) {
                description += `Movement detected: ${movingObjects.join(', ')}. `;
            }

            // Analyze object trajectories
            for (const [objClass, trajectory] of Object.entries(objectTracker)) {
                if (trajectory.length > 5) {
                    const startPoint = trajectory[0];
                    const endPoint = trajectory[trajectory.length - 1];
                    const xDiff = endPoint.x - startPoint.x;
                    const yDiff = endPoint.y - startPoint.y;
                    const timeDiff = (endPoint.time - startPoint.time) / 1000; // in seconds
                    const speed = Math.sqrt(xDiff * xDiff + yDiff * yDiff) / timeDiff;
                    const direction = Math.atan2(yDiff, xDiff) * 180 / Math.PI;

                    const speedDescription = getSpeedDescription(speed);
                    const directionDescription = getDirectionDescription(direction);
                    const positionDescription = getPositionDescription(endPoint.x, endPoint.y, canvas.width, canvas.height);

                    description += `${objClass} is moving ${speedDescription} ${directionDescription} in the ${positionDescription} of the screen. `;
                }
            }

            // Add pose recognition and behavior monitoring description
            if (poses && poses.length > 0) {
                description += `Detected ${poses.length} person(s). `;
                poses.forEach((pose, index) => {
                    const keypoints = pose.keypoints;
                    const leftWrist = keypoints.find(kp => kp.part === 'leftWrist');
                    const rightWrist = keypoints.find(kp => kp.part === 'rightWrist');
                    const nose = keypoints.find(kp => kp.part === 'nose');

                    if (leftWrist && rightWrist && nose) {
                        if (leftWrist.position.y < nose.position.y && rightWrist.position.y < nose.position.y) {
                            description += `Person ${index + 1} is raising both hands. `;
                            gestureHistory.push('raising hands');
                        } else if (Math.abs(leftWrist.position.x - rightWrist.position.x) > 100) {
                            description += `Person ${index + 1} might be waving. `;
                            gestureHistory.push('waving');
                        }
                    }

                    // Check for loitering in restricted areas
                    restrictedAreas.forEach((area, areaIndex) => {
                        if (nose.position.x > area.x && nose.position.x < area.x + area.width &&
                            nose.position.y > area.y && nose.position.y < area.y + area.height) {
                            description += `ALERT: Person ${index + 1} is in restricted area ${areaIndex + 1}. `;
                        }
                    });

                    // Add behavior description
                    if (behaviors[index]) {
                        description += `Person ${index + 1} is ${behaviors[index]}. `;
                    }
                });
            }

            // Analyze gesture history
            if (gestureHistory.length > 5) {
                const recentGestures = gestureHistory.slice(-5);
                if (recentGestures.filter(g => g === 'waving').length >= 3) {
                    description += 'Continuous waving detected. ';
                } else if (recentGestures.filter(g => g === 'raising hands').length >= 3) {
                    description += 'Repeated hand raising detected. ';
                }
                gestureHistory = gestureHistory.slice(-10); // Keep only last 10 gestures
            }

            // Add anomaly detection description
            if (anomalies && anomalies.length > 0) {
                description += `Statistical anomalies detected: ${anomalies.join(', ')}. `;
            }

            // Add motion detection and behavior classification description
            if (motions && motions.length > 0) {
                motions.forEach(motion => {
                    const speedDescription = getSpeedDescription(motion.speed);
                    description += `${motion.object} is ${motion.behavior} at ${speedDescription} speed. `;
                });
            }

            // Add face analysis results
            if (faceResults && faceResults.length > 0) {
                description += `Detected ${faceResults.length} face(s). `;
                faceResults.forEach((face, index) => {
                    description += `Face ${index + 1}: `;
                    if (face.age) description += `Estimated age: ${Math.round(face.age)}. `;
                    if (face.gender) description += `Estimated gender: ${face.gender}. `;
                    if (face.expressions) {
                        const topExpression = Object.entries(face.expressions).reduce((a, b) => a[1] > b[1] ? a : b);
                        description += `Dominant expression: ${topExpression[0]}. `;
                    }
                });
            }

            // Add vehicle classification information
            objects.forEach(async (obj) => {
                if (obj.class === 'car' || obj.class === 'truck' || obj.class === 'bus') {
                    const vehicleType = await classifyVehicle(obj);
                    description += `Detected a ${vehicleType} (${obj.class}). `;
                }
            });


            prevObjects = objects;
            return description.trim();
        }

        function getSpeedDescription(speed) {
            if (speed < 1) return "stationary";
            if (speed < 3) return "slowly";
            if (speed < 7) return "at medium speed";
            if (speed < 15) return "quickly";
            return "very quickly";
        }

        function getDirectionDescription(degrees) {
            if (degrees < 0) degrees += 360;
            if (degrees >= 315 || degrees < 45) return "upwards";
            if (degrees >= 45 && degrees < 135) return "to the right";
            if (degrees >= 135 && degrees < 225) return "downwards";
            return "to the left";
        }

        function getPositionDescription(x, y, width, height) {
            const xThird = width / 3;
            const yThird = height / 3;
            
            let vertical = y < yThird ? "top" : (y < 2 * yThird ? "middle" : "bottom");
            let horizontal = x < xThird ? "left" : (x < 2 * xThird ? "center" : "right");
            
            return `${vertical}-${horizontal}`;
        }

        async function loadModels() {
            try {
                log('Loading COCO-SSD model...');
                objectDetectionModel = await cocoSsd.load();
                log('COCO-SSD model loaded successfully');

                log('Loading PoseNet model...');
                poseNetModel = await posenet.load();
                log('PoseNet model loaded successfully');

                log('Loading Face-API.js models...');
                const modelPath = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/weights/';

                log('Loading SSD MobileNetv1 face detection model...');
                await faceapi.nets.ssdMobilenetv1.loadFromUri(modelPath + 'ssd_mobilenetv1_model-weights_manifest.json');
                log('SSD MobileNetv1 face detection model loaded successfully');

                log('Loading Face Landmark Detection model...');
                await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath + 'face_landmark_68_model-weights_manifest.json');
                log('Face Landmark Detection model loaded successfully');

                log('Loading Face Recognition model...');
                await faceapi.nets.faceRecognitionNet.loadFromUri(modelPath + 'face_recognition_model-weights_manifest.json');
                log('Face Recognition model loaded successfully');

                log('Loading Face Expression Detection model...');
                await faceapi.nets.faceExpressionNet.loadFromUri(modelPath + 'face_expression_model-weights_manifest.json');
                log('Face Expression Detection model loaded successfully');

                log('Loading Age and Gender Estimation model...');
                await faceapi.nets.ageGenderNet.loadFromUri(modelPath + 'age_gender_model-weights_manifest.json');
                log('Age and Gender Estimation model loaded successfully');

                log('All Face-API.js models loaded successfully');
                log('Initializing ml5.js OCR model...');
                await initOCRModel();
            } catch (error) {
                log('Error loading models: ' + error.message);
                console.error('Error details:', error);
            }
        }

        async function toggleCamera() {
            log('Toggling camera...');
            if (stream) {
                stopCamera();
            } else {
                log('Starting camera...');
                await startCamera();
            }
        }

        async function startCamera() {
            try {
                const constraints = {
                    video: {
                        facingMode: currentCameraFacingMode,
                        width: { ideal: 640 },
                        height: { ideal: 480 }
                    }
                };

                stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                await video.play();

                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                placeholderCanvas.width = video.videoWidth;
                placeholderCanvas.height = video.videoHeight;
                log(`Video dimensions: ${video.videoWidth}x${video.videoHeight}`);

                toggleCameraButton.textContent = 'Stop Camera';
                switchCameraButton.disabled = false;
                startDetectionButton.disabled = false;
                video.style.display = 'block';
                canvas.style.display = 'block';
                placeholderCanvas.style.display = 'none';
            } catch (error) {
                log('Error starting camera: ' + error.message);
            }
        }

        function stopCamera() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
                video.srcObject = null;
                toggleCameraButton.textContent = 'Start Camera';
                switchCameraButton.disabled = true;
                startDetectionButton.disabled = true;
                video.style.display = 'none';
                canvas.style.display = 'none';
                placeholderCanvas.style.display = 'block';
                placeholderCtx.drawImage(canvas, 0, 0);
                log('Camera stopped');
            }
        }

        function switchCamera() {
            log('Switching camera...');
            currentCameraFacingMode = currentCameraFacingMode === 'environment' ? 'user' : 'environment';
            if (stream) {
                stopCamera();
                startCamera();
            }
            log(`Switched to ${currentCameraFacingMode} camera`);
        }

        async function startDetection() {
            log('Starting detection...');
            if (!objectDetectionModel || !poseNetModel) {
                await loadModels();
            }

            isDetecting = !isDetecting;
            startDetectionButton.textContent = isDetecting ? 'Stop Detection' : 'Start Detection';

            if (isDetecting) {
                log('Object, pose, face, behavior, and statistical anomaly detection started');
                detectAll();
            } else {
                log('Object, pose, face, behavior, and statistical anomaly detection stopped');
            }
        }

        async function detectAll() {
            if (!isDetecting) return;

            log('Processing frame...');
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const mat = cv.matFromImageData(imageData);

            try {
                const objectPredictions = await objectDetectionModel.detect(video);
                const poses = await poseNetModel.estimateMultiplePoses(video);
                const motions = detectMotion(mat, objectPredictions);
                const anomalies = detectStatisticalAnomalies(motions);
                const faceResults = await detectFaces(video);
                const behaviors = analyzeBehavior(poses);

                log(`Detected ${objectPredictions.length} objects, ${poses.length} poses, ${motions.length} motions, ${anomalies.length} anomalies, ${faceResults.length} faces, and ${behaviors.length} behaviors`);

                objectPredictions.forEach(prediction => {
                    uniqueObjects.add(prediction.class);
                });
                updateObjectCount();

                let detectionText = '';
                for (const prediction of objectPredictions) {
                    const [x, y, width, height] = prediction.bbox;
                    const color = [255, 0, 0, 255]; // Red color for object bounding box

                    const point1 = new cv.Point(x, y);
                    const point2 = new cv.Point(x + width, y + height);
                    cv.rectangle(mat, point1, point2, color, 2);

                    const vehicleType = await classifyVehicle(prediction);
                    const text = `${prediction.class} (${vehicleType}) (${Math.round(prediction.score * 100)}%)`;
                    const org = new cv.Point(x, y - 5);
                    cv.putText(mat, text, org, cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1);

                    detectionText += `${text} at (${Math.round(x)}, ${Math.round(y)})\n`;

                    if (prediction.class === 'car' || prediction.class === 'truck' || prediction.class === 'bus') {
                        const licensePlateRegion = ctx.getImageData(x, y, width, height);
                        recognizeLicensePlate(licensePlateRegion);
                    }
                }

                // Draw pose keypoints and skeletons
                poses.forEach((pose, index) => {
                    const color = [0, 255, 0, 255]; // Green color for pose
                    pose.keypoints.forEach(keypoint => {
                        if (keypoint.score > 0.2) {
                            const { y, x } = keypoint.position;
                            cv.circle(mat, new cv.Point(x, y), 5, color, -1);
                        }
                    });

                    // Draw skeleton
                    const adjacentKeyPoints = posenet.getAdjacentKeyPoints(pose.keypoints, 0.2);
                    adjacentKeyPoints.forEach(keypoints => {
                        cv.line(mat, 
                            new cv.Point(keypoints[0].position.x, keypoints[0].position.y),
                            new cv.Point(keypoints[1].position.x, keypoints[1].position.y),
                            color, 2);
                    });

                    detectionText += `Person ${index + 1} detected with ${pose.keypoints.length} keypoints\n`;
                    detectionText += `Behavior: ${behaviors[index]}\n`;
                });

                // Draw restricted areas
                restrictedAreas.forEach((area, index) => {
                    cv.rectangle(mat, 
                        new cv.Point(area.x, area.y),
                        new cv.Point(area.x + area.width, area.y + area.height),
                        [255, 255, 0, 255], 2); // Yellow color for restricted areas
                    cv.putText(mat, `Restricted Area ${index + 1}`, 
                        new cv.Point(area.x, area.y - 5),
                        cv.FONT_HERSHEY_SIMPLEX, 0.5, [255, 255, 0, 255], 1);
                });

                // Draw face detection results
                faceResults.forEach((detection, index) => {
                    const { detection: { box }, age, gender, expressions } = detection;
                    const { x, y, width, height } = box;
                    const color = [0, 0, 255, 255]; // Blue color for face bounding box

                    cv.rectangle(mat, new cv.Point(x, y), new cv.Point(x + width, y + height), color, 2);

                    const text = `Face ${index + 1}: ${Math.round(age)}y ${gender}`;
                    cv.putText(mat, text, new cv.Point(x, y - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1);

                    detectionText += `${text}\n`;
                });

                cv.imshow(canvas, mat);
                detectionResultsDiv.textContent = detectionText;

                log('Updating NLP description...');
                const nlpDescription = generateNLPDescription(objectPredictions, poses, anomalies, motions, faceResults, behaviors);
                if (nlpDescription) {
                    updateNLPDescription(nlpDescription);
                }
            } catch (error) {
                log(`Error during detection: ${error.name} - ${error.message}`);
                console.error('Error stack:', error.stack);
                if (error instanceof TypeError) {
                    log('TypeError detected. Check if all required models and APIs are properly loaded.');
                } else if (error instanceof ReferenceError) {
                    log('ReferenceError detected. Check for undefined variables or functions.');
                } else {
                    log('Unknown error type. Check the console for more details.');
                }
            } finally {
                mat.delete();
            }

            requestAnimationFrame(detectAll);
        }

        function detectStatisticalAnomalies(motions) {
            if (!arguments || arguments.length === 0) {
                throw new Error(`${arguments.callee.name} called with no arguments`);
            }
            try {
                const anomalies = [];
                motions.forEach(motion => {
                    if (!speedHistory[motion.object]) {
                        speedHistory[motion.object] = [];
                    }
                    speedHistory[motion.object].push(motion.speed);

                    // Keep only the last 50 speed records
                    if (speedHistory[motion.object].length > 50) {
                        speedHistory[motion.object].shift();
                    }

                    if (speedHistory[motion.object].length > 10) {
                        const speeds = speedHistory[motion.object];
                        const mean = ss.mean(speeds);
                        const stdDev = ss.standardDeviation(speeds);
                        const zScore = Math.abs((motion.speed - mean) / stdDev);

                        if (zScore > 2) {
                            anomalies.push(`Unusual ${motion.object} speed`);
                        }
                    }
                });

                return anomalies;
            } catch (error) {
                log(`Error in ${arguments.callee.name}: ${error.name} - ${error.message}`);
                console.error(`Error stack for ${arguments.callee.name}:`, error.stack);
                throw error;
            }
        }

        function detectMotion(mat, objects) {
            if (!arguments || arguments.length === 0) {
                throw new Error(`${arguments.callee.name} called with no arguments`);
            }
            try {
                const motions = [];
                const prevFrame = new cv.Mat();
                cv.cvtColor(mat, prevFrame, cv.COLOR_RGBA2GRAY);

                if (motionHistory.length > 0) {
                    const prevMat = motionHistory[motionHistory.length - 1];
                    const flow = new cv.Mat();
                    cv.calcOpticalFlowFarneback(prevMat, prevFrame, flow, 0.5, 3, 15, 3, 5, 1.2, 0);

                    objects.forEach(obj => {
                        const [x, y, width, height] = obj.bbox.map(Math.round);
                        const roi = flow.roi(new cv.Rect(x, y, width, height));
                        const meanFlow = cv.mean(roi);
                        const speed = Math.sqrt(meanFlow[0] * meanFlow[0] + meanFlow[1] * meanFlow[1]);

                        let behavior = 'standing';
                        if (speed > 5) behavior = 'walking';
                        if (speed > 10) behavior = 'running';

                        motions.push({ object: obj.class, behavior, speed });

                        roi.delete();
                    });

                    flow.delete();
                }

                motionHistory.push(prevFrame);
                if (motionHistory.length > 2) {
                    motionHistory.shift().delete();
                }

                return motions;
            } catch (error) {
                log(`Error in ${arguments.callee.name}: ${error.name} - ${error.message}`);
                console.error(`Error stack for ${arguments.callee.name}:`, error.stack);
                throw error;
            }
        }

        async function detectFaces(inputVideo) {
            if (!arguments || arguments.length === 0) {
                throw new Error(`${arguments.callee.name} called with no arguments`);
            }
            try {
                const detections = await faceapi.detectAllFaces(inputVideo, new faceapi.SsdMobilenetv1Options())
                    .withFaceLandmarks()
                    .withFaceExpressions()
                    .withAgeAndGender();

                return detections;
            } catch (error) {
                log(`Error during face detection: ${error.name} - ${error.message}`);
                console.error(`Error stack for ${arguments.callee.name}:`, error.stack);
                return [];
            }
        }

        function analyzeBehavior(poses) {
            if (!arguments || arguments.length === 0) {
                throw new Error(`${arguments.callee.name} called with no arguments`);
            }
            try {
                return poses.map((pose, index) => {
                    const keypoints = pose.keypoints;
                    const nose = keypoints.find(kp => kp.part === 'nose');
                    const leftShoulder = keypoints.find(kp => kp.part === 'leftShoulder');
                    const rightShoulder = keypoints.find(kp => kp.part === 'rightShoulder');
                    const leftHip = keypoints.find(kp => kp.part === 'leftHip');
                    const rightHip = keypoints.find(kp => kp.part === 'rightHip');
                    const leftKnee = keypoints.find(kp => kp.part === 'leftKnee');
                    const rightKnee = keypoints.find(kp => kp.part === 'rightKnee');

                    if (!nose || !leftShoulder || !rightShoulder || !leftHip || !rightHip || !leftKnee || !rightKnee) {
                        return 'unknown';
                    }

                    const verticalSpeed = calculateVerticalSpeed(nose, index);
                    const hipKneeAngle = calculateAngle(leftHip, leftKnee, rightKnee);

                    let behavior = 'standing';

                    if (verticalSpeed > 20) {
                        behavior = 'jumping';
                    } else if (verticalSpeed > 10) {
                        behavior = 'running';
                    } else if (verticalSpeed > 2) {
                        behavior = 'walking';
                    } else if (hipKneeAngle < 90) {
                        behavior = 'sitting';
                    }

                    // Update behavior history
                    if (!behaviorHistory[index]) {
                        behaviorHistory[index] = [];
                    }
                    behaviorHistory[index].push(behavior);
                    if (behaviorHistory[index].length > 10) {
                        behaviorHistory[index].shift();
                    }

                    // Smooth out behavior classification
                    const recentBehaviors = behaviorHistory[index];
                                        const dominantBehavior = recentBehaviors.reduce((a, b) =>
                        recentBehaviors.filter(v => v === a).length >= recentBehaviors.filter(v => v === b).length ? a : b
                    );

                    return dominantBehavior;
                });
            } catch (error) {
                log(`Error in ${arguments.callee.name}: ${error.name} - ${error.message}`);
                console.error(`Error stack for ${arguments.callee.name}:`, error.stack);
                throw error;
            }
        }

        function calculateVerticalSpeed(keypoint, personIndex) {
            if (!keypoint) return 0;

            const currentTime = Date.now();
            const currentY = keypoint.position.y;

            if (!positionHistory[personIndex]) {
                positionHistory[personIndex] = { lastY: currentY, lastTime: currentTime };
                return 0;
            }

            const { lastY, lastTime } = positionHistory[personIndex];
            const timeDiff = (currentTime - lastTime) / 1000; // Convert to seconds
            const speed = Math.abs(currentY - lastY) / timeDiff;

            positionHistory[personIndex] = { lastY: currentY, lastTime: currentTime };

            return speed;
        }

        function calculateAngle(pointA, pointB, pointC) {
            const vectorAB = { x: pointB.position.x - pointA.position.x, y: pointB.position.y - pointA.position.y };
            const vectorBC = { x: pointC.position.x - pointB.position.x, y: pointC.position.y - pointB.position.y };

            const dotProduct = vectorAB.x * vectorBC.x + vectorAB.y * vectorBC.y;
            const magnitudeAB = Math.sqrt(vectorAB.x * vectorAB.x + vectorAB.y * vectorAB.y);
            const magnitudeBC = Math.sqrt(vectorBC.x * vectorBC.x + vectorBC.y * vectorBC.y);

            const angle = Math.acos(dotProduct / (magnitudeAB * magnitudeBC));
            return angle * (180 / Math.PI); // Convert to degrees
        }

        video = document.getElementById('video');
        canvas = document.getElementById('canvas');
        ctx = canvas.getContext('2d');
        placeholderCanvas = document.getElementById('placeholder-canvas');
        placeholderCtx = placeholderCanvas.getContext('2d');

        toggleCameraButton.addEventListener('click', toggleCamera);
        switchCameraButton.addEventListener('click', switchCamera);
        startDetectionButton.addEventListener('click', startDetection);

        // Initialize buttons
        switchCameraButton.disabled = true;
        startDetectionButton.disabled = true;

        // Expand functionality for log and nlpDescription
        document.querySelectorAll('.expand-btn').forEach(btn => {
            btn.addEventListener('click', function() {
                const parentDiv = this.closest('.log-container');
                parentDiv.classList.toggle('expanded');
                this.textContent = parentDiv.classList.contains('expanded') ? 'Collapse' : 'Expand';
            });
        });

        // Load OpenCV.js
        function onOpenCvReady() {
            log('OpenCV.js is ready');
            loadModels();
        }

        // Error handling wrapper function
        function errorHandler(func) {
            return async function(...args) {
                try {
                    return await func.apply(this, args);
                } catch (error) {
                    log(`Error in ${func.name}: ${error.name} - ${error.message}`);
                    console.error(`Error stack for ${func.name}:`, error.stack);
                }
            };
        }

        // Wrap main functions with error handler
        detectAll = errorHandler(detectAll);
        startDetection = errorHandler(startDetection);
        startCamera = errorHandler(startCamera);
        loadModels = errorHandler(loadModels);

        // Performance optimization using Web Worker for heavy computations
        const worker = new Worker(URL.createObjectURL(new Blob([`
            self.onmessage = function(e) {
                const { func, args } = e.data;
                if (func === 'detectStatisticalAnomalies') {
                    const anomalies = detectStatisticalAnomalies(args.motions);
                    self.postMessage({ func, result: anomalies });
                }
            }

            function detectStatisticalAnomalies(motions) {
                // Implementation of detectStatisticalAnomalies function
                const anomalies = [];
                const speedHistory = {};
                motions.forEach(motion => {
                    if (!speedHistory[motion.object]) {
                        speedHistory[motion.object] = [];
                    }
                    speedHistory[motion.object].push(motion.speed);

                    // Keep only the last 50 speed records
                    if (speedHistory[motion.object].length > 50) {
                        speedHistory[motion.object].shift();
                    }

                    if (speedHistory[motion.object].length > 10) {
                        const speeds = speedHistory[motion.object];
                        const mean = speeds.reduce((a, b) => a + b) / speeds.length;
                        const variance = speeds.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / speeds.length;
                        const stdDev = Math.sqrt(variance);
                        const zScore = Math.abs((motion.speed - mean) / stdDev);

                        if (zScore > 2) {
                            anomalies.push(`Unusual ${motion.object} speed`);
                        }
                    }
                });
                return anomalies;
            }
        `], { type: 'text/javascript' })));

        worker.onmessage = function(e) {
            const { func, result } = e.data;
            if (func === 'detectStatisticalAnomalies') {
                // Handle the result of statistical anomaly detection
                log(`Statistical anomalies detected: ${result.join(', ')}`);
            }
        };

        // Initialize the application
        loadModels().then(() => {
            log('Application initialized successfully');
        }).catch(error => {
            log('Error initializing application: ' + error.message);
            console.error('Error details:', error);
        });
    </script>
</body>
</html>
        
