<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Real-time Object, Pose, Face, Behavior, Text, and Statistical Anomaly Detection</title>
    <script src="https://unpkg.com/react@17/umd/react.production.min.js"></script>
    <script src="https://unpkg.com/react-dom@17/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
    <script src="https://cdn.jsdelivr.net/npm/simple-statistics@7.8.0/dist/simple-statistics.min.js"></script>
    <script src="https://docs.opencv.org/4.x/opencv.js" async></script>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js"></script>
    <script src='https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js'></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f0f0f0;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
        }
        h1 {
            text-align: center;
            color: #333;
        }
        .video-container {
            position: relative;
            width: 100%;
            max-width: 640px;
            margin: 0 auto;
            aspect-ratio: 4 / 3;
            background-color: #000;
        }
        #video, #canvas, #placeholder-canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: contain;
        }
        .controls {
            display: flex;
            justify-content: center;
            margin-top: 20px;
            gap: 10px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 5px;
        }
        button:hover {
            background-color: #45a049;
        }
        #total_obj, #detectionResults, #log, #nlpDescription {
            margin-top: 20px;
            padding: 10px;
            background-color: white;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        #total_obj {
            font-size: 18px;
            font-weight: bold;
            text-align: center;
        }
        #log, #nlpDescription {
            position: relative;
            white-space: pre-wrap;
            font-family: monospace;
            font-size: 14px;
            overflow-y: auto;
            padding-bottom: 30px;
            max-height: 200px;
        }
        .expand-btn {
            position: absolute;
            bottom: 5px;
            right: 5px;
            background-color: rgba(0, 0, 0, 0.5);
            color: white;
            border: none;
            padding: 5px 10px;
            cursor: pointer;
            font-size: 12px;
            border-radius: 3px;
            z-index: 10;
        }
        .expanded {
            max-height: none !important;
        }
        .log-container {
            position: relative;
        }
        .log-content {
            overflow-y: auto;
        }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        function App() {
            const [isDetecting, setIsDetecting] = useState(false);
            const [cameraActive, setCameraActive] = useState(false);
            const [currentCameraFacingMode, setCurrentCameraFacingMode] = useState('environment');
            const [uniqueObjects, setUniqueObjects] = useState(new Set());
            const [detectionResults, setDetectionResults] = useState('');
            const [nlpDescription, setNlpDescription] = useState([]);
            const [logMessages, setLogMessages] = useState([]);

            const videoRef = useRef(null);
            const canvasRef = useRef(null);
            const placeholderCanvasRef = useRef(null);
            const streamRef = useRef(null);
            const objectDetectionModelRef = useRef(null);
            const poseNetModelRef = useRef(null);
            const tesseractWorkerRef = useRef(null);
            const prevObjectsRef = useRef([]);
            const objectTrackerRef = useRef({});
            const gestureHistoryRef = useRef([]);
            const motionHistoryRef = useRef([]);
            const speedHistoryRef = useRef({});
            const behaviorHistoryRef = useRef([]);
            const positionHistoryRef = useRef([]);

            const restrictedAreas = [
                { x: 100, y: 100, width: 200, height: 150 },
                { x: 400, y: 300, width: 150, height: 100 }
            ];

            useEffect(() => {
                loadModels();
                return () => {
                    if (tesseractWorkerRef.current) {
                        tesseractWorkerRef.current.terminate();
                    }
                };
            }, []);

            function log(message) {
                console.log(message);
                setLogMessages(prev => [{time: new Date().toLocaleTimeString(), message}, ...prev.slice(0, 49)]);
            }

            async function loadModels() {
                try {
                    if (!WebAssembly) {
                        log('WebAssembly is not supported by this browser.');
                        return;
                    }

                    log('Loading COCO-SSD model...');
                    objectDetectionModelRef.current = await cocoSsd.load();
                    log('COCO-SSD model loaded successfully');

                    log('Loading PoseNet model...');
                    poseNetModelRef.current = await posenet.load();
                    log('PoseNet model loaded successfully');

                    log('Loading Face-API.js models...');
                    const modelPath = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/weights/';

                    log('Loading SSD MobileNetv1 face detection model...');
                    await faceapi.nets.ssdMobilenetv1.loadFromUri(modelPath);
                    log('SSD MobileNetv1 face detection model loaded successfully');

                    log('Loading Face Landmark Detection model...');
                    await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath);
                    log('Face Landmark Detection model loaded successfully');

                    log('Loading Face Recognition model...');
                    await faceapi.nets.faceRecognitionNet.loadFromUri(modelPath);
                    log('Face Recognition model loaded successfully');

                    log('Loading Face Expression Detection model...');
                    await faceapi.nets.faceExpressionNet.loadFromUri(modelPath);
                    log('Face Expression Detection model loaded successfully');

                    log('Loading Age and Gender Estimation model...');
                    await faceapi.nets.ageGenderNet.loadFromUri(modelPath);
                    log('Age and Gender Estimation model loaded successfully');

                    log('All Face-API.js models loaded successfully');

                    log('Initializing Tesseract worker...');
                    await initializeTesseractWorker();
                    log('Tesseract.js worker initialized successfully');
                } catch (error) {
                    log('Error loading models: ' + error.message);
                    console.error('Error details:', error);
                }
            }

            async function initializeTesseractWorker() {
                try {
                    log('Initializing Tesseract worker...');
                    tesseractWorkerRef.current = await Tesseract.createWorker();
                    await tesseractWorkerRef.current.load();
                    await tesseractWorkerRef.current.loadLanguage('eng');
                    await tesseractWorkerRef.current.initialize('eng');
                    log('Tesseract worker initialized successfully');
                } catch (error) {
                    log('Error initializing Tesseract worker: ' + error.message);
                    console.error('Tesseract initialization error details:', error);
                }
            }

            async function resetTesseractWorker() {
                if (tesseractWorkerRef.current) {
                    await tesseractWorkerRef.current.terminate();
                }
                await initializeTesseractWorker();
            }

            async function toggleCamera() {
                if (streamRef.current) {
                    stopCamera();
                } else {
                    await startCamera();
                }
            }

            async function startCamera() {
                try {
                    const constraints = {
                        video: {
                            facingMode: currentCameraFacingMode,
                            width: { ideal: 640 },
                            height: { ideal: 480 }
                        }
                    };

                    streamRef.current = await navigator.mediaDevices.getUserMedia(constraints);
                    videoRef.current.srcObject = streamRef.current;
                    await videoRef.current.play();

                    canvasRef.current.width = videoRef.current.videoWidth;
                    canvasRef.current.height = videoRef.current.videoHeight;
                    placeholderCanvasRef.current.width = videoRef.current.videoWidth;
                    placeholderCanvasRef.current.height = videoRef.current.videoHeight;
                    log(`Video dimensions: ${videoRef.current.videoWidth}x${videoRef.current.videoHeight}`);

                    setCameraActive(true);
                } catch (error) {
                    log('Error starting camera: ' + error.message);
                }
            }

            function stopCamera() {
                if (streamRef.current) {
                    streamRef.current.getTracks().forEach(track => track.stop());
                    streamRef.current = null;
                    videoRef.current.srcObject = null;
                    setCameraActive(false);
                    const placeholderCtx = placeholderCanvasRef.current.getContext('2d');
                    placeholderCtx.drawImage(canvasRef.current, 0, 0);
                    log('Camera stopped');
                }
            }

            function switchCamera() {
                setCurrentCameraFacingMode(prev => prev === 'environment' ? 'user' : 'environment');
                if (streamRef.current) {
                    stopCamera();
                    startCamera();
                }
                log(`Switched to ${currentCameraFacingMode} camera`);
            }

            async function startDetection() {
                if (!objectDetectionModelRef.current || !poseNetModelRef.current) {
                    await loadModels();
                }

                setIsDetecting(prev => !prev);
            }

            useEffect(() => {
                if (isDetecting) {
                    log('Object, pose, face, behavior, text, and statistical anomaly detection started');
                    detectAll();
                } else {
                    log('Object, pose, face, behavior, text, and statistical anomaly detection stopped');
                }
            }, [isDetecting]);

            async function detectAll() {
                if (!isDetecting) return;

                const ctx = canvasRef.current.getContext('2d');
                ctx.drawImage(videoRef.current, 0, 0, canvasRef.current.width, canvasRef.current.height);
                const imageData = ctx.getImageData(0, 0, canvasRef.current.width, canvasRef.current.height);
                if (!imageData || !imageData.data.length) {
                    log('Invalid image data from canvas');
                    return;
                }

                // Resize the image for Tesseract
                const scale = 0.5;  // Reduce size by 50%
                const resizedCanvas = document.createElement('canvas');
                resizedCanvas.width = canvasRef.current.width * scale;
                resizedCanvas.height = canvasRef.current.height * scale;
                const resizedCtx = resizedCanvas.getContext('2d');
                resizedCtx.drawImage(videoRef.current, 0, 0, resizedCanvas.width, resizedCanvas.height);
                const resizedImageData = resizedCtx.getImageData(0, 0, resizedCanvas.width, resizedCanvas.height);

                const mat = cv.matFromImageData(imageData);

                try {
                    const objectPredictions = await objectDetectionModelRef.current.detect(videoRef.current);
                    const poses = await poseNetModelRef.current.estimateMultiplePoses(videoRef.current);
                    const motions = detectMotion(mat, objectPredictions);
                    const anomalies = detectStatisticalAnomalies(motions);
                    const faceResults = await detectFaces(videoRef.current);
                    const behaviors = analyzeBehavior(poses);
                    const extractedText = await extractTextFromImage(resizedImageData);
                    const vehicleNumbers = await detectVehicleNumbers(objectPredictions, resizedImageData);

                    log(`Detected ${objectPredictions.length} objects, ${poses.length} poses, ${motions.length} motions, ${anomalies.length} anomalies, ${faceResults.length} faces, ${behaviors.length} behaviors, ${extractedText ? 'text' : 'no text'}, and ${vehicleNumbers.length} vehicle numbers`);

                    setUniqueObjects(prev => {
                        const newSet = new Set(prev);
                        objectPredictions.forEach(prediction => newSet.add(prediction.class));
                        return newSet;
                    });

                    let detectionText = '';
                    objectPredictions.forEach((prediction, index) => {
                        const [x, y, width, height] = prediction.bbox;
                        const color = [255, 0, 0, 255];

                        const point1 = new cv.Point(x, y);
                        const point2 = new cv.Point(x + width, y + height);
                        cv.rectangle(mat, point1, point2, color, 2);

                        const text = `${prediction.class} (${Math.round(prediction.score * 100)}%)`;
                        const org = new cv.Point(x, y - 5);
                        cv.putText(mat, text, org, cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1);

                        detectionText += `${index + 1}. ${text} at (${Math.round(x)}, ${Math.round(y)})\n`;
                    });

                    poses.forEach((pose, index) => {
                        const color = [0, 255, 0, 255];
                        pose.keypoints.forEach(keypoint => {
                            if (keypoint.score > 0.2) {
                                const { y, x } = keypoint.position;
                                cv.circle(mat, new cv.Point(x, y), 5, color, -1);
                            }
                        });

                        const adjacentKeyPoints = posenet.getAdjacentKeyPoints(pose.keypoints, 0.2);
                        adjacentKeyPoints.forEach(keypoints => {
                            cv.line(mat, 
                                new cv.Point(keypoints[0].position.x, keypoints[0].position.y),
                                new cv.Point(keypoints[1].position.x, keypoints[1].position.y),
                                color, 2);
                        });

                        detectionText += `Person ${index + 1} detected with ${pose.keypoints.length} keypoints\n`;
                        detectionText += `Behavior: ${behaviors[index]}\n`;
                    });

                    restrictedAreas.forEach((area, index) => {
                        cv.rectangle(mat, 
                            new cv.Point(area.x, area.y),
                            new cv.Point(area.x + area.width, area.y + area.height),
                            [255, 255, 0, 255], 2);
                        cv.putText(mat, `Restricted Area ${index + 1}`, 
                            new cv.Point(area.x, area.y - 5),
                            cv.FONT_HERSHEY_SIMPLEX, 0.5, [255, 255, 0, 255], 1);
                    });

                    faceResults.forEach((detection, index) => {
                        const { detection: { box }, age, gender, expressions } = detection;
                        const { x, y, width, height } = box;
                        const color = [0, 0, 255, 255];

                        cv.rectangle(mat, new cv.Point(x, y), new cv.Point(x + width, y + height), color, 2);

                        const text = `Face ${index + 1}: ${Math.round(age)}y ${gender}`;
                        cv.putText(mat, text, new cv.Point(x, y - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1);

                        detectionText += `${text}\n`;
                    });

                    if (extractedText) {
                        detectionText += `Extracted Text: ${extractedText}\n`;
                    }

                    if (vehicleNumbers.length > 0) {
                        detectionText += `Vehicle Numbers: ${vehicleNumbers.join(', ')}\n`;
                    }

                    cv.imshow(canvasRef.current, mat);
                    setDetectionResults(detectionText);

                    const nlpDescription = generateNLPDescription(objectPredictions, poses, anomalies, motions, faceResults, behaviors, extractedText, vehicleNumbers);
                    if (nlpDescription) {
                        setNlpDescription(prev => [{time: new Date().toLocaleTimeString(), description: nlpDescription}, ...prev.slice(0, 49)]);
                    }
                } catch (error) {
                    log('Error during detection: ' + error.message);
                } finally {
                    mat.delete();
                }

                requestAnimationFrame(detectAll);
            }

            function detectStatisticalAnomalies(motions) {
                const anomalies = [];
                motions.forEach(motion => {
                    if (!speedHistoryRef.current[motion.object]) {
                        speedHistoryRef.current[motion.object] = [];
                    }
                    speedHistoryRef.current[motion.object].push(motion.speed);

                    if (speedHistoryRef.current[motion.object].length > 50) {
                        speedHistoryRef.current[motion.object].shift();
                    }

                    if (speedHistoryRef.current[motion.object].length > 10) {
                        const speeds = speedHistoryRef.current[motion.object];
                        const mean = ss.mean(speeds);
                        const stdDev = ss.standardDeviation(speeds);
                        const zScore = Math.abs((motion.speed - mean) / stdDev);

                        if (zScore > 2) {
                            anomalies.push(`Unusual ${motion.object} speed`);
                        }
                    }
                });

                return anomalies;
            }

            function detectMotion(mat, objects) {
                const motions = [];
                const prevFrame = new cv.Mat();
                cv.cvtColor(mat, prevFrame, cv.COLOR_RGBA2GRAY);

                if (motionHistoryRef.current.length > 0) {
                    const prevMat = motionHistoryRef.current[motionHistoryRef.current.length - 1];
                    const flow = new cv.Mat();
                    cv.calcOpticalFlowFarneback(prevMat, prevFrame, flow, 0.5, 3, 15, 3, 5, 1.2, 0);

                    objects.forEach(obj => {
                        const [x, y, width, height] = obj.bbox.map(Math.round);
                        const roi = flow.roi(new cv.Rect(x, y, width, height));
                        const meanFlow = cv.mean(roi);
                        const speed = Math.sqrt(meanFlow[0] * meanFlow[0] + meanFlow[1] * meanFlow[1]);

                        let behavior = 'standing';
                        if (speed > 5) behavior = 'walking';
                        if (speed > 10) behavior = 'running';

                        motions.push({ object: obj.class, behavior, speed });

                        roi.delete();
                    });

                    flow.delete();
                }

                motionHistoryRef.current.push(prevFrame);
                if (motionHistoryRef.current.length > 2) {
                    motionHistoryRef.current.shift().delete();
                }

                return motions;
            }

            async function detectFaces(inputVideo) {
                try {
                    const detections = await faceapi.detectAllFaces(inputVideo, new faceapi.SsdMobilenetv1Options())
                        .withFaceLandmarks()
                        .withFaceExpressions()
                        .withAgeAndGender();

                    return detections;
                } catch (error) {
                    log('Error during face detection: ' + error.message);
                    return [];
                }
            }

            function analyzeBehavior(poses) {
                return poses.map((pose, index) => {
                    const keypoints = pose.keypoints;
                    const nose = keypoints.find(kp => kp.part === 'nose');
                    const leftShoulder = keypoints.find(kp => kp.part === 'leftShoulder');
                    const rightShoulder = keypoints.find(kp => kp.part === 'rightShoulder');
                    const leftHip = keypoints.find(kp => kp.part === 'leftHip');
                    const rightHip = keypoints.find(kp => kp.part === 'rightHip');
                    const leftKnee = keypoints.find(kp => kp.part === 'leftKnee');
                    const rightKnee = keypoints.find(kp => kp.part === 'rightKnee');

                    if (!nose || !leftShoulder || !rightShoulder || !leftHip || !rightHip || !leftKnee || !rightKnee) {
                        return 'unknown';
                    }

                    const verticalSpeed = calculateVerticalSpeed(nose, index);
                    const hipKneeAngle = calculateAngle(leftHip, leftKnee, rightKnee);

                    let behavior = 'standing';

                    if (verticalSpeed > 20) {
                        behavior = 'jumping';
                    } else if (verticalSpeed > 10) {
                        behavior = 'running';
                    } else if (verticalSpeed > 2) {
                        behavior = 'walking';
                    } else if (hipKneeAngle < 90) {
                        behavior = 'sitting';
                    }

                    if (!behaviorHistoryRef.current[index]) {
                        behaviorHistoryRef.current[index] = [];
                    }
                    behaviorHistoryRef.current[index].push(behavior);
                    if (behaviorHistoryRef.current[index].length > 10) {
                        behaviorHistoryRef.current[index].shift();
                    }

                    const recentBehaviors = behaviorHistoryRef.current[index];
                    const dominantBehavior = recentBehaviors.reduce((a, b) =>
                        recentBehaviors.filter(v => v === a).length >= recentBehaviors.filter(v => v === b).length ? a : b
                    );

                    return dominantBehavior;
                });
            }

            function calculateVerticalSpeed(keypoint, personIndex) {
                if (!keypoint) return 0;

                const currentTime = Date.now();
                const currentY = keypoint.position.y;

                if (!positionHistoryRef.current[personIndex]) {
                    positionHistoryRef.current[personIndex] = { lastY: currentY, lastTime: currentTime };
                    return 0;
                }

                const { lastY, lastTime } = positionHistoryRef.current[personIndex];
                const timeDiff = (currentTime - lastTime) / 1000;
                const speed = Math.abs(currentY - lastY) / timeDiff;

                positionHistoryRef.current[personIndex] = { lastY: currentY, lastTime: currentTime };

                return speed;
            }

            function calculateAngle(pointA, pointB, pointC) {
                const vectorAB = { x: pointB.position.x - pointA.position.x, y: pointB.position.y - pointA.position.y };
                const vectorBC = { x: pointC.position.x - pointB.position.x, y: pointC.position.y - pointB.position.y };

                const dotProduct = vectorAB.x * vectorBC.x + vectorAB.y * vectorBC.y;
                const magnitudeAB = Math.sqrt(vectorAB.x * vectorAB.x + vectorAB.y * vectorAB.y);
                const magnitudeBC = Math.sqrt(vectorBC.x * vectorBC.x + vectorBC.y * vectorBC.y);

                const angle = Math.acos(dotProduct / (magnitudeAB * magnitudeBC));
                return angle * (180 / Math.PI);
            }

            async function extractTextFromImage(imageData) {
                if (!imageData || !imageData.data || !imageData.data.length) {
                    log('Invalid image data for text extraction');
                    return '';
                }
                try {
                    log('Starting text extraction...');
                    const { data: { text } } = await tesseractWorkerRef.current.recognize(imageData);
                    log('Text extraction completed');
                    return text.trim();
                } catch (error) {
                    log(`Tesseract error: ${error.message}`);
                    console.error('Text extraction error details:', error);
                    await resetTesseractWorker();  // Reset worker on failure
                    return '';
                }
            }

            async function detectVehicleNumbers(objects, imageData) {
                const vehicleNumbers = [];
                for (const obj of objects) {
                    if (obj.class === 'car' || obj.class === 'truck') {
                        const [x, y, width, height] = obj.bbox.map(Math.round);
                        const licensePlateRegion = new ImageData(
                            imageData.data.slice((y * imageData.width + x) * 4, ((y + height) * imageData.width + (x + width)) * 4),
                            width,
                            height
                        );
                        
                        try {
                            const { data: { text } } = await tesseractWorkerRef.current.recognize(licensePlateRegion);
                            const potentialPlateNumber = text.replace(/[^A-Z0-9]/g, '');
                            if (potentialPlateNumber.length >= 5) {
                                vehicleNumbers.push(potentialPlateNumber);
                            }
                        } catch (error) {
                            log(`Error during vehicle number detection: ${error.message}`);
                            console.error('Vehicle number detection error details:', error);
                            await resetTesseractWorker();  // Reset worker on failure
                        }
                    }
                }
                return vehicleNumbers;
            }

            function generateNLPDescription(objects, poses, anomalies, motions, faceResults, behaviors, extractedText, vehicleNumbers) {
                let description = '';
                const newObjects = objects.filter(obj => !prevObjectsRef.current.some(prevObj => prevObj.class === obj.class));
                const disappearedObjects = prevObjectsRef.current.filter(prevObj => !objects.some(obj => obj.class === prevObj.class));

                if (newObjects.length > 0) {
                    description += `New objects detected: ${newObjects.map(obj => obj.class).join(', ')}. `;
                }

                if (disappearedObjects.length > 0) {
                    description += `Objects no longer visible: ${disappearedObjects.map(obj => obj.class).join(', ')}. `;
                }

                const movingObjects = [];
                objects.forEach(obj => {
                    const prevObj = prevObjectsRef.current.find(pObj => pObj.class === obj.class);
                    if (prevObj) {
                        const xDiff = obj.bbox[0] - prevObj.bbox[0];
                        const yDiff = obj.bbox[1] - prevObj.bbox[1];
                        if (Math.abs(xDiff) > 10 || Math.abs(yDiff) > 10) {
                            const xDirection = xDiff > 0 ? 'right' : 'left';
                            const yDirection = yDiff > 0 ? 'down' : 'up';
                            movingObjects.push(`${obj.class} moving ${xDirection} and ${yDirection}`);
                        }
                    }

                    if (!objectTrackerRef.current[obj.class]) {
                        objectTrackerRef.current[obj.class] = [];
                    }
                    objectTrackerRef.current[obj.class].push({x: obj.bbox[0], y: obj.bbox[1], time: Date.now()});
                    if (objectTrackerRef.current[obj.class].length > 10) {
                        objectTrackerRef.current[obj.class].shift();
                    }
                });

                if (movingObjects.length > 0) {
                    description += `Movement detected: ${movingObjects.join(', ')}. `;
                }

                for (const [objClass, trajectory] of Object.entries(objectTrackerRef.current)) {
                    if (trajectory.length > 5) {
                        const startPoint = trajectory[0];
                        const endPoint = trajectory[trajectory.length - 1];
                        const xDiff = endPoint.x - startPoint.x;
                        const yDiff = endPoint.y - startPoint.y;
                        const timeDiff = (endPoint.time - startPoint.time) / 1000;
                        const speed = Math.sqrt(xDiff * xDiff + yDiff * yDiff) / timeDiff;
                        const direction = Math.atan2(yDiff, xDiff) * 180 / Math.PI;

                        const speedDescription = getSpeedDescription(speed);
                        const directionDescription = getDirectionDescription(direction);
                        const positionDescription = getPositionDescription(endPoint.x, endPoint.y, canvasRef.current.width, canvasRef.current.height);

                        description += `${objClass} is moving ${speedDescription} ${directionDescription} in the ${positionDescription} of the screen. `;
                    }
                }

                if (poses && poses.length > 0) {
                    description += `Detected ${poses.length} person(s). `;
                    poses.forEach((pose, index) => {
                        const keypoints = pose.keypoints;
                        const leftWrist = keypoints.find(kp => kp.part === 'leftWrist');
                        const rightWrist = keypoints.find(kp => kp.part === 'rightWrist');
                        const nose = keypoints.find(kp => kp.part === 'nose');

                        if (leftWrist && rightWrist && nose) {
                            if (leftWrist.position.y < nose.position.y && rightWrist.position.y < nose.position.y) {
                                description += `Person ${index + 1} is raising both hands. `;
                                gestureHistoryRef.current.push('raising hands');
                            } else if (Math.abs(leftWrist.position.x - rightWrist.position.x) > 100) {
                                description += `Person ${index + 1} might be waving. `;
                                gestureHistoryRef.current.push('waving');
                            }
                        }

                        restrictedAreas.forEach((area, areaIndex) => {
                            if (nose.position.x > area.x && nose.position.x < area.x + area.width &&
                                nose.position.y > area.y && nose.position.y < area.y + area.height) {
                                description += `ALERT: Person ${index + 1} is in restricted area ${areaIndex + 1}. `;
                            }
                        });

                        if (behaviors[index]) {
                            description += `Person ${index + 1} is ${behaviors[index]}. `;
                        }
                    });
                }

                if (gestureHistoryRef.current.length > 5) {
                    const recentGestures = gestureHistoryRef.current.slice(-5);
                    if (recentGestures.filter(g => g === 'waving').length >= 3) {
                        description += 'Continuous waving detected. ';
                    } else if (recentGestures.filter(g => g === 'raising hands').length >= 3) {
                        description += 'Repeated hand raising detected. ';
                    }
                    gestureHistoryRef.current = gestureHistoryRef.current.slice(-10);
                }

                if (anomalies && anomalies.length > 0) {
                    description += `Statistical anomalies detected: ${anomalies.join(', ')}. `;
                }

                if (motions && motions.length > 0) {
                    motions.forEach(motion => {
                        const speedDescription = getSpeedDescription(motion.speed);
                        description += `${motion.object} is ${motion.behavior} at ${speedDescription} speed. `;
                    });
                }

                if (faceResults && faceResults.length > 0) {
                    description += `Detected ${faceResults.length} face(s). `;
                    faceResults.forEach((face, index) => {
                        description += `Face ${index + 1}: `;
                        if (face.age) description += `Estimated age: ${Math.round(face.age)}. `;
                        if (face.gender) description += `Estimated gender: ${face.gender}. `;
                        if (face.expressions) {
                            const topExpression = Object.entries(face.expressions).reduce((a, b) => a[1] > b[1] ? a : b);
                            description += `Dominant expression: ${topExpression[0]}. `;
                        }
                    });
                }

                if (extractedText && extractedText.length > 0) {
                    description += `Extracted text: ${extractedText}. `;
                }

                if (vehicleNumbers && vehicleNumbers.length > 0) {
                    description += `Detected vehicle numbers: ${vehicleNumbers.join(', ')}. `;
                }

                prevObjectsRef.current = objects;
                return description.trim();
            }

            function getSpeedDescription(speed) {
                if (speed < 1) return "stationary";
                if (speed < 3) return "slowly";
                if (speed < 7) return "at medium speed";
                if (speed < 15) return "quickly";
                return "very quickly";
            }

            function getDirectionDescription(degrees) {
                if (degrees < 0) degrees += 360;
                if (degrees >= 315 || degrees < 45) return "upwards";
                if (degrees >= 45 && degrees < 135) return "to the right";
                if (degrees >= 135 && degrees < 225) return "downwards";
                return "to the left";
            }

            function getPositionDescription(x, y, width, height) {
                const xThird = width / 3;
                const yThird = height / 3;
                
                let vertical = y < yThird ? "top" : (y < 2 * yThird ? "middle" : "bottom");
                let horizontal = x < xThird ? "left" : (x < 2 * xThird ? "center" : "right");
                
                return `${vertical}-${horizontal}`;
            }

            return (
                <div className="container">
                    <h1>Advanced Real-time Object, Pose, Face, Behavior, Text, and Statistical Anomaly Detection</h1>
                    <div className="video-container">
                        <video ref={videoRef} style={{display: cameraActive ? 'block' : 'none'}} playsInline></video>
                        <canvas ref={canvasRef} style={{display: cameraActive ? 'block' : 'none'}}></canvas>
                        <canvas ref={placeholderCanvasRef} style={{display: cameraActive ? 'none' : 'block'}}></canvas>
                    </div>
                    <div className="controls">
                        <button onClick={toggleCamera}>{cameraActive ? 'Stop Camera' : 'Start Camera'}</button>
                        <button onClick={switchCamera} disabled={!cameraActive}>Switch Camera</button>
                        <button onClick={startDetection} disabled={!cameraActive}>{isDetecting ? 'Stop Detection' : 'Start Detection'}</button>
                    </div>
                    <div id="total_obj">Total Detected <span id="obj_count">{uniqueObjects.size}</span> objects</div>
                    <div id="detectionResults">{detectionResults}</div>
                    <div id="nlpDescription" className="log-container">
                        <div className="log-content">
                            {nlpDescription.map((item, index) => (
                                <p key={index}>{item.time}: {item.description}</p>
                            ))}
                        </div>
                        <button className="expand-btn">Expand</button>
                    </div>
                    <div id="log" className="log-container">
                        <div className="log-content">
                            {logMessages.map((item, index) => (
                                <div key={index}>{item.time}: {item.message}</div>
                            ))}
                        </div>
                        <button className="expand-btn">Expand</button>
                    </div>
                </div>
            );
        }

        ReactDOM.render(<App />, document.getElementById('root'));
    </script>
</body>
</html>
