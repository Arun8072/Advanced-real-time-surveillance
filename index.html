<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real-time Object Detection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f0f0f0;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        h1 {
            text-align: center;
            color: #333;
        }

        .video-container {
            position: relative;
            width: 100%;
            max-width: 640px;
            margin: 0 auto;
        }

        #video, #canvas {
            width: 100%;
            height: auto;
            max-height: 480px;
            display: block;
        }

        #canvas {
            position: absolute;
            top: 0;
            left: 0;
        }

        .controls {
            display: flex;
            justify-content: center;
            margin-top: 20px;
            gap: 10px;
        }

        button {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 5px;
        }

        button:hover {
            background-color: #45a049;
        }

        #detectionResults, #log, #nlpDescription {
            margin-top: 20px;
            padding: 10px;
            background-color: white;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }

        #log {
            white-space: pre-wrap;
            font-family: monospace;
            font-size: 14px;
            max-height: 200px;
            overflow-y: auto;
        }

        #nlpDescription p {
            margin: 5px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Real-time Object Detection</h1>
        <div class="video-container">
            <video id="video" playsinline></video>
            <canvas id="canvas"></canvas>
        </div>
        <div class="controls">
            <button id="startCamera">Start Camera</button>
            <button id="switchCamera">Switch Camera</button>
            <button id="startDetection">Start Detection</button>
        </div>
        <div id="detectionResults"></div>
        <div id="nlpDescription"></div>
        <div id="log"></div>
        <span id="obj_count"></span>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script src="https://docs.opencv.org/4.x/opencv.js" onload="onOpenCvReady();" async></script>
    <script>
        let video, canvas, ctx, model, stream;
        let isDetecting = false;
        let currentCameraFacingMode = 'environment';
        const objCountSpan = document.getElementById('obj_count');
        const nlpDescriptionDiv = document.getElementById('nlpDescription');
        let prevObjectCount = 0;
        let prevObjects = [];

        const startCameraButton = document.getElementById('startCamera');
        const switchCameraButton = document.getElementById('switchCamera');
        const startDetectionButton = document.getElementById('startDetection');
        const detectionResultsDiv = document.getElementById('detectionResults');
        const logDiv = document.getElementById('log');

        function log(message) {
            console.log(message);
        }

        function updateObjectCount(count) {
            if (count !== prevObjectCount) {
                objCountSpan.textContent = count;
                prevObjectCount = count;
            }
        }

        function generateNLPDescription(objects) {
            let description = '';
            const newObjects = objects.filter(obj => !prevObjects.some(prevObj => prevObj.class === obj.class));
            const disappearedObjects = prevObjects.filter(prevObj => !objects.some(obj => obj.class === prevObj.class));

            if (newObjects.length > 0) {
                description += `Detected new ${newObjects.map(obj => obj.class).join(', ')}.`;
            }

            if (disappearedObjects.length > 0) {
                description += ` ${disappearedObjects.map(obj => obj.class).join(', ')} disappeared.`;
            }

            objects.forEach(obj => {
                const prevObj = prevObjects.find(pObj => pObj.class === obj.class);
                if (prevObj) {
                    const xDiff = obj.bbox[0] - prevObj.bbox[0];
                    const yDiff = obj.bbox[1] - prevObj.bbox[1];
                    if (Math.abs(xDiff) > 10 || Math.abs(yDiff) > 10) {
                        const xDirection = xDiff > 0 ? 'right' : 'left';
                        const yDirection = yDiff > 0 ? 'down' : 'up';
                        description += ` ${obj.class} moving ${xDirection} and ${yDirection}.`;
                    }
                }
            });

            prevObjects = objects;
            return description.trim();
        }


        async function loadModel() {
            log('Loading COCO-SSD model...');
            model = await cocoSsd.load();
            log('Model loaded successfully');
        }

        async function startCamera() {
            try {
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }

                const constraints = {
                    video: {
                        facingMode: currentCameraFacingMode,
                        width: { ideal: 640 },
                        height: { ideal: 480 }
                    }
                };

                stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                await video.play();

                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                log(`Video dimensions: ${video.videoWidth}x${video.videoHeight}`);

                startCameraButton.disabled = true;
                switchCameraButton.disabled = false;
                startDetectionButton.disabled = false;
            } catch (error) {
                log('Error starting camera: ' + error.message);
            }
        }

        function switchCamera() {
            currentCameraFacingMode = currentCameraFacingMode === 'environment' ? 'user' : 'environment';
            startCamera();
        }

        async function startDetection() {
            if (!model) {
                await loadModel();
            }

            isDetecting = !isDetecting;
            startDetectionButton.textContent = isDetecting ? 'Stop Detection' : 'Start Detection';

            if (isDetecting) {
                detectObjects();
            }
        }

        async function detectObjects() {
            if (!isDetecting) return;

            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const mat = cv.matFromImageData(imageData);

            try {
                const predictions = await model.detect(video);
                log(`Detected ${predictions.length} objects`);

                updateObjectCount(predictions.length);

                let detectionText = '';
                predictions.forEach((prediction, index) => {
                    const [x, y, width, height] = prediction.bbox;
                    const color = [255, 0, 0, 255]; // Red color for bounding box

                    const point1 = new cv.Point(x, y);
                    const point2 = new cv.Point(x + width, y + height);
                    cv.rectangle(mat, point1, point2, color, 2);

                    const text = `${prediction.class} (${Math.round(prediction.score * 100)}%)`;
                    const org = new cv.Point(x, y - 5);
                    cv.putText(mat, text, org, cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1);

                    detectionText += `${index + 1}. ${text} at (${Math.round(x)}, ${Math.round(y)})\n`;
                });

                cv.imshow(canvas, mat);
                detectionResultsDiv.textContent = detectionText;

                const nlpDescription = generateNLPDescription(predictions);
                if (nlpDescription) {
                    const descriptionElement = document.createElement('p');
                    descriptionElement.textContent = nlpDescription;
                    nlpDescriptionDiv.insertBefore(descriptionElement, nlpDescriptionDiv.firstChild);
                    if (nlpDescriptionDiv.childElementCount > 5) {
                        nlpDescriptionDiv.removeChild(nlpDescriptionDiv.lastChild);
                    }
                }
            } catch (error) {
                log('Error during object detection: ' + error.message);
            } finally {
                mat.delete();
            }

            requestAnimationFrame(detectObjects);
        }

        video = document.getElementById('video');
        canvas = document.getElementById('canvas');
        ctx = canvas.getContext('2d');

        startCameraButton.addEventListener('click', startCamera);
        switchCameraButton.addEventListener('click', switchCamera);
        startDetectionButton.addEventListener('click', startDetection);

        // Initialize buttons
        switchCameraButton.disabled = true;
        startDetectionButton.disabled = true;

        // Load OpenCV.js
        function onOpenCvReady() {
            log('OpenCV.js is ready');
            loadModel();
        }
    </script>
</body>
</html>
