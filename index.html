<!DOCTYPE html>
<html>
<head>
  <title>Real-time Video Analysis</title>
  <style>
    #video {
      width: 100%;
      aspect-ratio: 16 / 9;
    }
    #description {
      border: 1px solid gray;
      padding: 10px;
    }
    #canvas {
      display: none; /* Hide canvas for direct processing */
    }
  </style>
</head>
<body>
  <button id="startButton" style="width: 100%;">Start Camera</button>
  <button id="switchCamera" style="width: 50%;">Switch Camera</button>
  <video id="video" autoplay muted></video>
  <div id="description">Detected Objects:</div>
  <canvas id="canvas"></canvas>
  <button id="monitorButton">Monitor</button>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://docs.opencv.org/4.x/opencv.js"></script>
  <script>
    const startButton = document.getElementById('startButton');
    const switchCameraButton = document.getElementById('switchCamera');
    const video = document.getElementById('video');
    const description = document.getElementById('description');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    let facingMode = 'user'; // Default to front camera
    let stream = null;
    let model = null;

    // Load the COCO-SSD model
    cocoSsd.load().then(loadedModel => {
      model = loadedModel;
      console.log("COCO-SSD model loaded.");
    }).catch(err => console.error("Error loading COCO-SSD model:", err));

    startButton.addEventListener('click', () => {
      console.log('Starting camera...');
      navigator.mediaDevices.getUserMedia({ video: { facingMode: facingMode } })
        .then(newStream => {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
          }
          stream = newStream;
          video.srcObject = stream;
          video.play();

          video.addEventListener('loadeddata', () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            cv.onRuntimeInitialized = () => {
              console.log("OpenCV.js initialized.");
              processVideo();
            };
          });
        })
        .catch(error => console.error("Error accessing camera:", error));
    });

    switchCameraButton.addEventListener('click', () => {
      console.log("Switching camera...");
      facingMode = facingMode === "user" ? "environment" : "user";
      startButton.click();
    });

    function processVideo() {
      const cap = new cv.VideoCapture(video);
      const frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);

      function analyzeFrame() {
        if (!model) {
          console.error("Model not loaded yet.");
          return;
        }

        cap.read(frame);

        // Draw the frame on canvas
        cv.imshow(canvas, frame);

        // Convert frame to TensorFlow tensor
        const tensor = tf.browser.fromPixels(canvas);
        model.detect(tensor).then(predictions => {
          tensor.dispose(); // Dispose tensor to save memory

          // Draw detections
          predictions.forEach(prediction => {
            const [x, y, width, height] = prediction.bbox;
            cv.rectangle(frame, new cv.Point(x, y), new cv.Point(x + width, y + height), [0, 255, 0, 255], 2);
            cv.putText(frame, prediction.class, new cv.Point(x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, [255, 0, 0, 255], 2);
          });

          cv.imshow(canvas, frame);

          // Update description
          description.textContent = predictions.map(p => p.class).join(", ") || "No objects detected.";
        });

        // Call the next frame
        requestAnimationFrame(analyzeFrame);
      }

      requestAnimationFrame(analyzeFrame);
    }
  </script>
</body>
</html>
